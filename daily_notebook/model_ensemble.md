# 模型融合方法

在Kaggle比赛中，模型融合是一种提高精度指标的常用方法。常用的模型融合方法有以下几种：

### 1.投票法

投票法是最简单的模型融合方法。假设对于一个二分类问题，有3个基础模型，那么就采取投票制的方法，投票多者确定为最终的分类。

### 2.平均法

平均法同时适用于分类和回归问题。它的思想是对多个模型的输出结果（logits或probability）取平均获得最终的预测结果。它的一个改进方法是加权平均法，即根据模型的效果好坏，分别赋予每个模型不同的权重，然后再计算平均。举个例子，比如A、B、C三种基本模型，模型效果进行排名，假设排名分别是1，2，3，那么给这三个模型赋予的权值分别是3/6、2/6、1/6。

### 3.Bagging

Bagging采用有放回的采样方式，采样得到子训练集，在子训练集上进行模型训练，重复多次分别训练不同的模型，最后采用融合方法融合预测结果。具体步骤分为两步：

（1）训练多个子模型，每次

* 有放回的抽样，建立子训练集
* 训练子模型

（2）融合

采用投票法或平均法

### 4.Boosting

Bagging算法可以并行处理，而Boosting的思想是一种迭代的方法，每一次训练的时候都更加关心分类错误的样例，给这些分类错误的样例增加更大的权重，下一次迭代的目标就是能够更容易辨别出上一轮分类错误的样例。最终将这些弱分类器进行加权相加。

### 5.Stacking/Stacked Generalization

Stacking相对复杂一些，通常训练两层结构，首先训练多个子模型，作为第一层，然后将第一层的输出当成第二层stacker model的输入，训练第二层模型。其中的思想相当于为每个基模型训练出对应的权重来进行模型融合。

假设采用三种模型融合，基模型为M1, M2, M3。具体的方法如下：

* 按K折交叉验证方法划分K折数据集，将某一折P当成验证集，其他折当训练集，训练多个基模型M1。利用每个M1模型对折P预测，则最终得到整个训练集的预测结果R1并保存。对测试集预测时，每个M1模型都进行预测，并对预测结果取平均，记为RT1。
* 重复训练M2、M3，得到R2、R3和RT2、RT3，将（R1，R2，R3）（RT1，RT2，RT3）合并
* 将（R1，R2，R3）当成第二层模型的输入，输出为训练集的标签，训练stack model
* 利用stack model对（RT1，RT2，RT3）进行预测得到融合后的结果

### 6.Blending

Blending方法与Stacking类似，只不过将K折交叉验证方法替换为one-out方法。在训练时，保存部分数据作验证集，其他当成训练集，分别训练基模型，然后再训练stack model。